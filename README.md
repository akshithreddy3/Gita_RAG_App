# Gita_RAG_App

# ğŸ“œ Bhagavad Gita RAG Tutor (LangChain + Chroma + Ollama + Streamlit)

> A local, privacy-friendly AI tutor that answers your questions only from the **Bhagavad Gita PDF**, using a Retrieval-Augmented Generation (RAG) pipeline built with **LangChain**, **Chroma**, **Hugging Face Embeddings**, and **Ollama** running locally on your Mac.

---

## ğŸŒŸ Overview

This project demonstrates how to build an **end-to-end RAG application** that:
- Loads and processes long PDFs (like the *Bhagavad Gita*)
- Splits and embeds the text using **sentence-transformers**
- Stores vector embeddings in a **Chroma** database
- Retrieves relevant passages dynamically
- Generates grounded, context-aware answers via a **local LLM** (using Ollama)
- Presents a beautiful chat interface powered by **Streamlit**

Everything runs locally â€” **no cloud APIs or data leaks**.

---

## ğŸ§  Tech Stack

| Category | Tools / Frameworks |
|-----------|--------------------|
| **LLM & Framework** | [LangChain](https://python.langchain.com/), [Ollama](https://ollama.ai/) |
| **Vector Database** | [Chroma](https://www.trychroma.com/) |
| **Embeddings** | [Hugging Face Sentence Transformers](https://www.sbert.net/) |
| **Frontend / UI** | [Streamlit](https://streamlit.io/) |
| **Language / Environment** | Python 3.11+, Conda / venv |
| **Data** | Bhagavad Gita (PDF, ~900 pages) |

---

## ğŸ—ï¸ Project Structure

```bash
rag_gita/
â”œâ”€â”€ app.py               # Streamlit chat interface
â”œâ”€â”€ ingest.py            # PDF loader â†’ chunking â†’ embeddings â†’ Chroma
â”œâ”€â”€ rag_chain.py         # Retriever + LLM chain setup
â”œâ”€â”€ prompts.py           # System + human prompt templates
â”œâ”€â”€ utils.py             # Helper functions
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ .env                 # Config (paths, models, retrieval params)
â””â”€â”€ data/
    â””â”€â”€ bhagavad_gita.pdf
```


---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/Gita_RAG_App.git
cd Gita_RAG_App

### 2ï¸âƒ£ Create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate

### 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

### 4ï¸âƒ£ Install & run Ollama
brew install ollama
ollama serve     # keep this terminal open
ollama pull mistral:7b
# or: ollama pull llama3.1:8b

### 5ï¸âƒ£ Configure environment variables (.env)
DOCS_DIR=./data
CHROMA_DIR=./chroma
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
OLLAMA_MODEL=mistral:7b
CHUNK_SIZE=1000
CHUNK_OVERLAP=150
TOP_K=4
MMR=true
SCORE_THRESHOLD=0.0

### 6ï¸âƒ£ Ingest your PDF
python ingest.py

### 7ï¸âƒ£ Launch the app
python -m streamlit run app.py

Then open the local URL (usually http://localhost:8501).


## ğŸ’¬ Example Queries
â€œWhat does the Gita say about doing oneâ€™s duty without attachment?â€

â€œHow can one control the mind and emotions?â€

â€œSummarize the concept of karma yoga.â€

â€œHow does Krishna define a wise person?â€

â€œExplain the meaning of renunciation.â€


## ğŸ§© Key Features
âœ… Completely Local â€“ No cloud API or key required
âœ… Grounded Answers â€“ Responses only from your PDF context
âœ… Customizable Models â€“ Swap between mistral:7b, llama3:8b, or phi3:3.8b
âœ… Fast Retrieval â€“ Optimized chunking & MMR search via Chroma
âœ… Beautiful UI â€“ Streamlit chat with expandable source citations


## ğŸ§° Troubleshooting
| Issue                       | Solution                                                   |
| --------------------------- | ---------------------------------------------------------- |
| `httpx.RemoteProtocolError` | Restart `ollama serve` or use smaller model (`mistral:7b`) |
| Model loading slow          | Set `num_ctx=2048`, `streaming=False` in `rag_chain.py`    |
| Duplicate embeddings        | Delete `/chroma` and re-run `python ingest.py`             |
| Scanned PDF (image-only)    | Add OCR with `pytesseract` or `unstructured` loader        |


## ğŸš€ Future Enhancements
| Feature                      | Description                                             |
| ---------------------------- | ------------------------------------------------------- |
| ğŸ§  **Conversational Memory** | Remember previous Q&As using `ConversationBufferMemory` |
| ğŸ“š **Verse Citations**       | Auto-detect and display â€œChapter:Verseâ€ references      |
| ğŸ’¬ **Voice Input / Output**  | Integrate `speech_recognition` + `gTTS`                 |
| ğŸ” **Multi-Book Support**    | Drop multiple PDFs â†’ unified knowledge base             |
| â˜ï¸ **Gemini/OpenAI Support** | Add toggle for cloud LLMs in `.env`                     |


## ğŸ§‘â€ğŸ’» Author
Akshith Reddy K
ğŸ“ Data Analyst / Data Engineer / AI-Enabled BI Professional
ğŸ”— LinkedIn â€¢ GitHub


## ğŸªª License
MIT License Â© 2025 Akshith Reddy K
Feel free to use, modify, and distribute with attribution.
